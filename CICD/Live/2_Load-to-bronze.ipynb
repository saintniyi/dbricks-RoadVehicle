{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "209a5a6d-8ac7-4034-b269-d786701e6b18",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Get environment name from widget and assign to variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46158663-a37d-4b4b-9a15-67770a68ec41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(name=\"env\", defaultValue='', label='Enter environment in lower case')\n",
    "env = dbutils.widgets.get(\"env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9421f18-9f85-416f-9f87-7691bf4edb2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "### Call common notebook to access shared variables and methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e878d2-a939-4e42-89c7-6d20efd4277e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./3_Common\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3e76cb1-45ed-4917-b21e-ea2af83e6fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating a read_Traffic_Data() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36afd650-39e0-4aac-ade2-6fbe99f14593",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_Traffic_Data():\n",
    "    \n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "    print(\"Reading the Raw Traffic Data :  \", end='')\n",
    "    \n",
    "    schema = StructType([\n",
    "    StructField(\"Record_ID\",IntegerType()),\n",
    "    StructField(\"Count_point_id\",IntegerType()),\n",
    "    StructField(\"Direction_of_travel\",StringType()),\n",
    "    StructField(\"Year\",IntegerType()),\n",
    "    StructField(\"Count_date\",StringType()),\n",
    "    StructField(\"hour\",IntegerType()),\n",
    "    StructField(\"Region_id\",IntegerType()),\n",
    "    StructField(\"Region_name\",StringType()),\n",
    "    StructField(\"Local_authority_name\",StringType()),\n",
    "    StructField(\"Road_name\",StringType()),\n",
    "    StructField(\"Road_Category_ID\",IntegerType()),\n",
    "    StructField(\"Start_junction_road_name\",StringType()),\n",
    "    StructField(\"End_junction_road_name\",StringType()),\n",
    "    StructField(\"Latitude\",DoubleType()),\n",
    "    StructField(\"Longitude\",DoubleType()),\n",
    "    StructField(\"Link_length_km\",DoubleType()),\n",
    "    StructField(\"Pedal_cycles\",IntegerType()),\n",
    "    StructField(\"Two_wheeled_motor_vehicles\",IntegerType()),\n",
    "    StructField(\"Cars_and_taxis\",IntegerType()),\n",
    "    StructField(\"Buses_and_coaches\",IntegerType()),\n",
    "    StructField(\"LGV_Type\",IntegerType()),\n",
    "    StructField(\"HGV_Type\",IntegerType()),\n",
    "    StructField(\"EV_Car\",IntegerType()),\n",
    "    StructField(\"EV_Bike\",IntegerType())\n",
    "    ])\n",
    "\n",
    "    rawTraffic_stream = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\",\"csv\")\n",
    "        .option('cloudFiles.schemaLocation',f'{checkpoint}/rawTrafficLoad/schemaInfer')\n",
    "        .option('header','true')\n",
    "        .schema(schema)\n",
    "        .load(landing + '/raw_traffic/')\n",
    "        .withColumn(\"Extract_Time\", current_timestamp()))\n",
    "    \n",
    "    print('Reading Succcess !!')\n",
    "    print('*******************')\n",
    "\n",
    "    return rawTraffic_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0752bf4b-426b-42a1-bcec-e1f750a69df1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating read_Road_Data() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9472b2a0-af2b-4ee3-9d1c-ede51e0f7bc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_Road_Data():\n",
    "\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "    print(\"Reading the Raw Roads Data :  \", end='')\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField('Road_ID',IntegerType()),\n",
    "        StructField('Road_Category_Id',IntegerType()),\n",
    "        StructField('Road_Category',StringType()),\n",
    "        StructField('Region_ID',IntegerType()),\n",
    "        StructField('Region_Name',StringType()),\n",
    "        StructField('Total_Link_Length_Km',DoubleType()),\n",
    "        StructField('Total_Link_Length_Miles',DoubleType()),\n",
    "        StructField('All_Motor_Vehicles',DoubleType())\n",
    "        \n",
    "        ])\n",
    "\n",
    "    rawRoads_stream = (spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\",\"csv\")\n",
    "        .option('cloudFiles.schemaLocation',f'{checkpoint}/rawRoadsLoad/schemaInfer')\n",
    "        .option('header','true')\n",
    "        .schema(schema)\n",
    "        .load(landing + '/raw_roads/')\n",
    "        )\n",
    "    \n",
    "    print('Reading Succcess !!')\n",
    "    print('*******************')\n",
    "\n",
    "    return rawRoads_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "884c71e1-c8af-4ee0-8ec0-bfea2017ad91",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating write_Traffic_Data(StreamingDF,environment) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "892a344f-9543-431f-9e9e-c91f04ea9c71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Traffic_Data(StreamingDF, environment):\n",
    "    print(f'Writing data to {environment}_catalog raw_traffic table :  ', end='' )\n",
    "    write_Stream = (StreamingDF.writeStream\n",
    "                    .format('delta')\n",
    "                    .option(\"checkpointLocation\", checkpoint + '/rawTrafficLoad/Checkpt')\n",
    "                    .outputMode('append')\n",
    "                    .queryName('rawTrafficWriteStream')\n",
    "                    .trigger(availableNow=True)\n",
    "                    .toTable(f\"`{environment}_catalog`.`bronze`.`raw_traffic`\"))\n",
    "    \n",
    "    write_Stream.awaitTermination()\n",
    "    print('Write Success')\n",
    "    print(\"****************************\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f03eee9b-d6aa-4a90-b37b-31b270e890c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Creating write_Road_Data(StreamingDF,environment) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710eb7c5-c1db-4c11-878b-f9aa6fecc101",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_Road_Data(StreamingDF,environment):\n",
    "    print(f'Writing data to {environment}_catalog raw_roads table :  ', end='' )\n",
    "    write_Data = (StreamingDF.writeStream\n",
    "                    .format('delta')\n",
    "                    .option(\"checkpointLocation\", checkpoint + '/rawRoadsLoad/Checkpt')\n",
    "                    .outputMode('append')\n",
    "                    .queryName('rawRoadsWriteStream')\n",
    "                    .trigger(availableNow=True)\n",
    "                    .toTable(f\"`{environment}_catalog`.`bronze`.`raw_roads`\"))\n",
    "    \n",
    "    write_Data.awaitTermination()\n",
    "    print('Write Success')\n",
    "    print(\"****************************\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9ab5f79-8f21-4f93-838a-b7dad2784a8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Calling read and Write Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4fd3573-e2a8-4380-aca7-2ef03ab930e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Reading the raw_traffic's data from landing to Bronze\n",
    "read_Df = read_Traffic_Data()\n",
    "\n",
    "## Reading the raw_roads's data from landing to Bronze\n",
    "read_roads = read_Road_Data()\n",
    "\n",
    "## Writing the raw_traffic's data from landing to Bronze\n",
    "write_Traffic_Data(read_Df, env)\n",
    "\n",
    "## Writing the raw_roads's data from landing to Bronze\n",
    "write_Road_Data(read_roads, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a519b4-d865-4bc4-b7a5-69ae5f88ca48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM `{env}_catalog`.`bronze`.`raw_traffic` Limit 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82295957-d4f8-461e-95ef-19c60e93e1a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM `{env}_catalog`.`bronze`.`raw_roads` Limit 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d252017-88ae-4b9d-867c-4bb6ed0a8741",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"Select max(Record_ID) from `{env}_catalog`.`bronze`.`raw_traffic`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c72dbdcd-f4e6-4bc2-96fa-3a4f93fd2df6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<br>\n",
    "\n",
    "To demostrate incremental auto loading, I will upload 'raw_traffic2' to the 'landing/raw_traffic' folder and observe the new RecordID count as well as the Extract_Time after I rerun the cell for reading & writing and the cell below to display the max record in the raw_traffic table. \n",
    "\n",
    "**Note**: _Autoloader automatically handles the incremental without any special code to do this_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9198744a-42ac-425e-9985-53a99229eab0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"Select max(Record_ID) from `{env}_catalog`.`bronze`.`raw_traffic`\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_Load-to-bronze",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "4197a557-be1b-4817-b208-5eaba9d352d7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Enter environment in lower case",
      "name": "env",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": "Enter environment in lower case",
      "name": "env",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
